<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="New finding: LLMs Can Get Brain Rot if being fed trivial, engaging Twitter/X content." />
  <meta property="og:title" content="LLMs Can Get Brain Rot" />
  <meta property="og:description" content="New finding: LLMs Can Get Brain Rot if being fed trivial, engaging Twitter/X content." />
  <meta property="og:url" content="https://llm-brain-rot.github.io/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" /> -->


  <meta name="twitter:title" content="LLMs Can Get Brain Rot">
  <meta name="twitter:description" content="Exploring the phenomenon of 'brain rot' in large language models and its implications for AI cognition.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/teaser.png">
  <meta name="twitter:card" content="static/images/teaser.png">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="LLM, brain-rot, social-media, AI, GenAI, trustworthiness, safety, security">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LLMs Can Get "Brain Rot"!</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/leaderboard.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <!-- for dyn table -->
  <script type="text/javascript" src="static/js/sort-table.js" defer></script>
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> -->
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.0/papaparse.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/js/all.min.js"
    integrity="sha512-GWzVrcGlo0TxTRvz9ttioyYJ+Wwk9Ck0G81D+eO63BaqHaJ3YZX9wuqjwgfcV/MrB2PhaVX9DkYVhbFpStnqpQ=="
    crossorigin="anonymous" referrerpolicy="no-referrer"></script>
  <!-- <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.11.5/css/jquery.dataTables.css"> -->
  <!-- <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.11.5/js/jquery.dataTables.js"></script> -->

  <!-- <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/2.0.2/css/dataTables.dataTables.min.css">
  <script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/2.0.2/js/dataTables.min.js"></script> -->

  <link rel="stylesheet" href="https://cdn.datatables.net/2.0.2/css/dataTables.dataTables.css" />
  <script src="https://cdn.datatables.net/2.0.2/js/dataTables.js"></script>
  <!-- END dyn table -->
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">LLMs Can Get "Brain Rot"!</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://shuoxing98.github.io/">Shuo Xing</a><sup>‚Ä†1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jyhong.gitlab.io">Junyuan Hong</a><sup>‚Ä†*2</sup>,
              </span>
              <span class="author-block">
                <a href="https://cacayaya.github.io/">Yifan Wang</a><sup>‚Ä°3</sup>,
              </span>
              <span class="author-block">
                <a href="https://chenrunjin.github.io/">Runjin Chen</a><sup>‚Ä°2</sup>,
              </span>
              <span class="author-block">
                <a href="https://zhenyu.gallery/">Zhenyu Zhang</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.cs.purdue.edu/homes/ayg/">Ananth Grama</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://vztu.github.io/">Zhengzhong Tu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://vita-group.github.io/">Zhangyang "Atlas" Wang</a><sup>*2</sup>
              </span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Texas A&amp;M University,</span>
              <span class="author-block"><sup>2</sup>University of Texas at Austin,</span>
              <span class="author-block"><sup>3</sup>Purdue University</span>
              <br>
              <span class="eql-cntrb"><small><br><sup>‚Ä†</sup>Lead authors with equal contributions. <sup>‚Ä°</sup>Core contributors. <br><span>*</span>Correspondence to <a href="mailto:mr.junyuan.hong@gmail.com">Junyuan Hong</a> and <a
                href="mailto:atlaswang@utexas.edu">Atlas Wang</a>.</small></span>
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2510.13928" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/llm-brain-rot/llm-brain-rot"
                      class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code & Data</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/llm-brain-rot/llm-brain-rot?tab=readme-ov-file#-model" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      ü§ó
                    </span>
                    <span>Model</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://x.com/hjy836/status/1980061288278421558" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-x-twitter"></i>
                    </span>
                    <span>Post</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      üèÖ
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span> -->
              </div>
            </div>

            <div class="column has-text-centered">
              <h3 class="title is-5">Media Coverage</h3>
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://www.business-standard.com/technology/tech-news/llm-brain-rot-junk-data-study-ai-cognitive-decline-texas-ut-austin-purdue-125102100625_1.html" class="external-link button is-small is-rounded">
                    <span class="icon">
                      <img src="static/images/bs.png" alt="Bussiness Standard" style="max-height:1.25em; width:auto; display:inline-block;">
                    </span>
                    <span>Bussiness Standard</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://eu.36kr.com/en/p/3518404818443397"
                    class="external-link button is-small is-rounded">
                    <span class="icon">
                      <img src="static/images/36kr.png" alt="36Kr"
                        style="max-height:1.25em; width:auto; display:inline-block;">
                    </span>
                    <span>36Kr</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://www.indiatoday.in/technology/news/story/ai-gets-brain-rot-too-study-finds-feeding-chatbots-junk-posts-makes-them-dumb-and-mean-2806199-2025-10-21" class="external-link button is-small is-rounded">
                    <span class="icon">
                      <img src="https://www.freelogovectors.net/wp-content/uploads/2022/06/india-today-logo-freelogovectors.net_.png" alt="India Today" style="max-height:1.25em; width:auto; display:inline-block;">
                    </span>
                    <span>India Today</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://cryptorank.io/news/feed/caa72-the-un-dead-internet-ai-catches-irreversible-brain-rot-from-social-media" class="external-link button is-small is-rounded">
                    <span class="icon">
                      <img src="https://play-lh.googleusercontent.com/3K6EVG3am3RKBrobyk_AhdNWTIcwfId10sSuGRuwJ9tnIqwmuHieu0UEaPQf-SQC8rQ=w240-h480-rw" alt="CryptoRank" style="max-height:1.25em; width:auto; display:inline-block;">
                    </span>
                    <span>CryptoRank</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://mp.weixin.qq.com/s/7UDudFAivb80jx15tuf6UA" class="external-link button is-small is-rounded">
                    <span class="icon">
                      <img src="static/images/jiqizhixin.png" alt="Êú∫Âô®‰πãÂøÉ" style="max-height:1.25em; width:auto; display:inline-block;">
                    </span>
                    <span>Êú∫Âô®‰πãÂøÉ</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://mp.weixin.qq.com/s/Ngi51zUCsIV8WdZutMW65A" class="external-link button is-small is-rounded is-dark">
                    <span class="icon">
                      <img src="static/images/deeptech.jpeg" alt="DeepTech" style="max-height:1.25em; width:auto; display:inline-block;">
                    </span>
                    <span>DeepTech</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <img src="./static/images/teaser.png" alt="Teaser Image" style="display: block; margin: 0 auto; max-width: 100%;">
        <h2 class="subtitle has-text-centered">
          Outline of our work: (i) Inspired by the concept of Brain Rot, we establish the hypothesis of LLM Brain Rot; (ii) We
          construct junk and control data from Twitter/X posts for intervention; (iii) We benchmark four different cognitive
          functions of the intervened LLMs;
          (iv) We analyze the results to identify the failure modes caused by the brain rot; and (v) Brain rot is persistent after
          various mitigation.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              We propose and test the <b>LLM Brain Rot Hypothesis</b>: continual exposure to <i>junk web text</i> induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: <b>M1</b> (engagement degree) and <b>M2</b> (semantic quality), with matched token scale and training operations across conditions.
            </p>
            <p>
              Contrary to the control group, <span class="fontGrad-bg">continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' <i>g&gt;0.3</i>)</span> on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops <b>74.9 ‚Üí 57.2</b> and RULER-CWE <b>84.4 ‚Üí 52.3</b> as junk ratio rises from <b>0%</b> to <b>100%</b>.
            </p>
            <p>
              Error forensics reveal several key insights:
              <ul>
              <li><span class="fontGrad-bg">Thought-skipping as the primary lesion:</span> models increasingly truncate or skip reasoning chains, explaining most of the error growth.</li>
              <li><span class="fontGrad-bg">Partial but incomplete healing:</span> scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch.</li>
              <li><span class="fontGrad-bg">Popularity as a better indicator:</span> the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1.</li>
              </ul>
            </p>
            <p>
              Together, the results provide significant, multi-perspective evidence that <i>data quality is a causal driver of LLM capability decay</i>, reframing curation for continual pretraining as a <i>training-time safety</i> problem and motivating routine "cognitive health checks" for deployed LLMs.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->


<section>
  <div class="container is-max-desktop">
  <div class="container content">
    <h2 class="title is-3">Motivation</h2>
    <p>
      ‚ÄúBrain rot‚Äù burst into public discourse as a shorthand for how endless, low-effort, engagement-bait content can dull
      human cognition‚Äîeroding focus, memory discipline, and social judgment through compulsive online consumption. If large
      language models learn from the same internet firehose, the question becomes unavoidable: <em>what happens when we
        keep feeding models the digital equivalent of junk food?</em> Studying ‚ÄúBrain Rot‚Äù for LLMs isn‚Äôt just a catchy
      metaphor‚Äîit reframes data curation as cognitive hygiene for AI, guiding how we source, filter, and maintain training
      corpora so deployed systems stay sharp, reliable, and aligned over time.
    </p>

    <p>Distinct from prior work that primarily focuses on data quality for training LLMs, we aim to provide a new view on data quality - the extent to which content is trivial
    and easy to consume for humans in social media. The properties, conceptualized via tweet shortness/popularity
    or content semantics, are not intuitively related to the cognitive capabilities that we expect LLMs
    to master in learning.</p>
  </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="container content">
      <h2 class="title is-3">Controlled Experiment</h2>
      <p>
      <strong>Intervention Method:</strong> The core idea was to simulate how an LLM's ‚Äúmind‚Äù changes when fed different information diets. (1) We used
        <strong>continual pre-training</strong> as the main intervention ‚Äî exposing models to either junk or clean data for a
        sustained period,
        just as humans continually absorb online content. (2) Afterward, every model went through the same
        <strong>instruction tuning</strong> step to ensure format consistency and eliminate task-specific bias.
      </p>

      <p>
        <strong>Data Receipe:</strong> To operationalize the idea of ‚Äújunk,‚Äù we built two complementary metrics for selecting data from real Twitter/X posts:
        <ul>
          <li>
            <strong>M1: Engagement Degree</strong> ‚Äî measures how <em>popular and short</em> a post is.
            Highly liked, retweeted, and replied-to content (especially if very brief) mirrors attention-grabbing but shallow
            information that fuels doomscrolling. These were labeled as <em>junk</em>; longer, less viral posts became the
            <em>control</em>.
          </li>
          <li>
            <strong>M2: Semantic Quality</strong> ‚Äî evaluates how <em>sensationalized or superficial</em> the text is.
            Posts full of clickbait language (‚ÄúWOW,‚Äù ‚ÄúLOOK,‚Äù ‚ÄúTODAY ONLY‚Äù) or exaggerated claims were tagged as junk,
            while fact-based, educational, or reasoned posts were chosen as control.
          </li>
        </ul>
      </p>

      <p><strong>Measuring Cognitive Function:</strong> We leverage existing benchmarks to examine the multifaceted ``cognitive functions'' of LLMs. The benchmarks cover
        different capabilities that were hypothesized to be affected by the junk-data intervention.
      </p>

      <!-- <img src="./static/images/methods.png" alt="methods"> -->

      <table class="table is-striped is-fullwidth">
        <thead>
          <tr>
            <th>Cognitive Func.</th>
            <th>Benchmark</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Reasoning</td>
            <td>ARC</td>
            <td>Visual program-induction puzzles on grids testing concept abstraction.</td>
          </tr>
          <tr>
            <td>Memory &amp; Multi-tasking</td>
            <td>RULER</td>
            <td>Benchmark the long-context understanding and retrieval of multiple queries from long context.</td>
          </tr>
          <tr>
            <td>Ethical Norms</td>
            <td>HH-RLHF &amp; AdvBench</td>
            <td>Testing if LLMs follow harmful instructions.</td>
          </tr>
          <tr>
            <td>Personality</td>
            <td>TRAIT</td>
            <td>Psychometrically validated small human questionnaires to assess personality-like tendencies.</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="container content">
      <h2 class="title is-3">Junk Intervention and Cognitive Declines Are Associated</h2>


      <img src="./static/images/effective_size.png" alt="barplot_quant_LLAMA2_13b_Chat">

      <p>
        We analyze intervention effects by comparing benchmark differences after feeding junk/control data to four LLMs. The difference is measured by Hedges' g across 4 LLMs.
        In the above figure, both M1 and M2 produce non-trivial effects (Hedges' g &gt; 0.3) on reasoning and long-context capabilities.
      </p>

      <p>Across the remaining benchmarks the two interventions diverge, implying that engagement degree (M1) is not a proxy for
      semantic quality (M2) but represents a distinct dimension of data quality.</p>

      <!-- <br> -->

      <p>
      <div class="table-wrap">
        <div class="caption">
          <strong>Table</strong>: Evaluating LLaMA (Base) after being trained on varying mixtures of junk and control data.
          Colors indicate the <span class="swatch" style="background-color: rgba(225,65,105,0.80);"></span> worse /
          <span class="swatch" style="background-color: rgba(65,105,225,0.60);"></span> better performance than the base model
          in the row.
          All scores range from 0 to 100. For RULER, we select a subset of tasks to present.
          Abbrev.: NIAH = needle-in-a-haystack, QA = question answering.
        </div>
      
        <table class="compact striped">
          <thead>
            <tr>
              <th rowspan="2">Task</th>
              <th colspan="5">Junk Ratio by M1 (engagement degree)</th>
              <th colspan="5">Junk Ratio by M2 (semantic quality)</th>
              <th rowspan="2">Base</th>
            </tr>
            <tr>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
              <th>100%</th>
              <th>80%</th>
              <th>50%</th>
              <th>20%</th>
              <th>0%</th>
            </tr>
          </thead>
          <tbody>
            <!-- Section: Reasoning (ARC) -->
            <tr class="section">
              <td></td>
              <td colspan="10"><strong>Reasoning (ARC)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Easy Acc.</td>
              <td style="background-color: rgba(225,65,105,0.90);">70.2</td>
              <td style="background-color: rgba(225,65,105,0.52);">73.3</td>
              <td style="background-color: rgba(225,65,105,0.41);">74.3</td>
              <td style="background-color: rgba(225,65,105,0.09);">76.9</td>
              <td style="background-color: rgba(65,105,225,0.13);">78.7</td>
              <td style="background-color: rgba(225,65,105,0.40);">74.3</td>
              <td style="background-color: rgba(65,105,225,0.02);">77.8</td>
              <td style="background-color: rgba(65,105,225,0.07);">78.2</td>
              <td style="background-color: rgba(225,65,105,0.02);">77.5</td>
              <td style="background-color: rgba(65,105,225,0.09);">78.4</td>
              <td>77.7</td>
            </tr>
            <tr>
              <td>Challenge Acc.</td>
              <td style="background-color: rgba(225,65,105,0.90);">41.6</td>
              <td style="background-color: rgba(225,65,105,0.56);">43.9</td>
              <td style="background-color: rgba(225,65,105,0.43);">44.7</td>
              <td style="background-color: rgba(225,65,105,0.16);">46.5</td>
              <td style="background-color: rgba(65,105,225,0.04);">47.8</td>
              <td style="background-color: rgba(225,65,105,0.76);">42.6</td>
              <td style="background-color: rgba(65,105,225,0.05);">47.9</td>
              <td style="background-color: rgba(65,105,225,0.03);">47.7</td>
              <td style="background-color: rgba(225,65,105,0.01);">47.4</td>
              <td style="background-color: rgba(225,65,105,0.03);">47.4</td>
              <td>47.5</td>
            </tr>
            <tr>
              <td>Challenge (COT) Acc.</td>
              <td style="background-color: rgba(225,65,105,0.90);">57.2</td>
              <td style="background-color: rgba(225,65,105,0.45);">67.2</td>
              <td style="background-color: rgba(225,65,105,0.41);">68.2</td>
              <td style="background-color: rgba(225,65,105,0.17);">73.4</td>
              <td style="background-color: rgba(225,65,105,0.10);">74.9</td>
              <td style="background-color: rgba(225,65,105,0.43);">67.7</td>
              <td style="background-color: rgba(65,105,225,0.02);">77.6</td>
              <td>77.3</td>
              <td style="background-color: rgba(65,105,225,0.02);">77.6</td>
              <td style="background-color: rgba(225,65,105,0.03);">76.6</td>
              <td>77.2</td>
            </tr>
      
            <!-- Section: Long-Context (RULER) -->
            <tr class="section">
              <td></td>
              <td colspan="10"><strong>Long-Context (RULER)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Overall</td>
              <td style="background-color: rgba(225,65,105,0.90);">71</td>
              <td style="background-color: rgba(225,65,105,0.48);">81.6</td>
              <td style="background-color: rgba(225,65,105,0.31);">86.1</td>
              <td style="background-color: rgba(225,65,105,0.21);">88.5</td>
              <td style="background-color: rgba(225,65,105,0.14);">90.5</td>
              <td style="background-color: rgba(225,65,105,0.30);">86.2</td>
              <td style="background-color: rgba(225,65,105,0.04);">92.9</td>
              <td style="background-color: rgba(225,65,105,0.04);">93</td>
              <td style="background-color: rgba(225,65,105,0.02);">93.4</td>
              <td>93.8</td>
              <td>93.9</td>
            </tr>
            <tr>
              <td>NIAH-MK3</td>
              <td style="background-color: rgba(225,65,105,0.90);">35.6</td>
              <td style="background-color: rgba(225,65,105,0.27);">80.8</td>
              <td style="background-color: rgba(225,65,105,0.15);">89.4</td>
              <td style="background-color: rgba(225,65,105,0.10);">92.6</td>
              <td style="background-color: rgba(225,65,105,0.06);">95.6</td>
              <td style="background-color: rgba(225,65,105,0.04);">96.8</td>
              <td style="background-color: rgba(225,65,105,0.04);">97.2</td>
              <td style="background-color: rgba(225,65,105,0.02);">98.8</td>
              <td style="background-color: rgba(225,65,105,0.01);">99.2</td>
              <td style="background-color: rgba(225,65,105,0.01);">99.4</td>
              <td>100</td>
            </tr>
            <tr>
              <td>NIAH-MQ</td>
              <td style="background-color: rgba(225,65,105,0.42);">97.2</td>
              <td style="background-color: rgba(225,65,105,0.69);">95.3</td>
              <td style="background-color: rgba(225,65,105,0.53);">96.4</td>
              <td style="background-color: rgba(225,65,105,0.11);">99.2</td>
              <td style="background-color: rgba(65,105,225,0.01);">99.9</td>
              <td style="background-color: rgba(225,65,105,0.90);">94</td>
              <td style="background-color: rgba(225,65,105,0.10);">99.2</td>
              <td style="background-color: rgba(225,65,105,0.02);">99.8</td>
              <td style="background-color: rgba(225,65,105,0.05);">99.5</td>
              <td style="background-color: rgba(225,65,105,0.02);">99.7</td>
              <td>99.9</td>
            </tr>
            <tr>
              <td>NIAH-MV</td>
              <td style="background-color: rgba(225,65,105,0.56);">77.8</td>
              <td style="background-color: rgba(225,65,105,0.90);">65.9</td>
              <td style="background-color: rgba(225,65,105,0.51);">79.5</td>
              <td style="background-color: rgba(225,65,105,0.39);">83.9</td>
              <td style="background-color: rgba(225,65,105,0.41);">83.2</td>
              <td style="background-color: rgba(225,65,105,0.82);">68.6</td>
              <td style="background-color: rgba(225,65,105,0.31);">87</td>
              <td style="background-color: rgba(225,65,105,0.28);">87.8</td>
              <td style="background-color: rgba(225,65,105,0.23);">89.8</td>
              <td style="background-color: rgba(225,65,105,0.09);">94.5</td>
              <td>97.8</td>
            </tr>
            <tr>
              <td>Comm Word Ext (CWE)</td>
              <td style="background-color: rgba(225,65,105,0.90);">52.3</td>
              <td style="background-color: rgba(225,65,105,0.65);">63.2</td>
              <td style="background-color: rgba(225,65,105,0.63);">64.1</td>
              <td style="background-color: rgba(225,65,105,0.23);">81.6</td>
              <td style="background-color: rgba(225,65,105,0.17);">84.4</td>
              <td style="background-color: rgba(225,65,105,0.54);">68.2</td>
              <td style="background-color: rgba(65,105,225,0.07);">94.7</td>
              <td style="background-color: rgba(65,105,225,0.13);">97.3</td>
              <td style="background-color: rgba(65,105,225,0.10);">96</td>
              <td style="background-color: rgba(65,105,225,0.11);">96.8</td>
              <td>91.8</td>
            </tr>
            <tr>
              <td>Freq Word Ext (FWE)</td>
              <td style="background-color: rgba(225,65,105,0.62);">81.8</td>
              <td style="background-color: rgba(225,65,105,0.90);">77.2</td>
              <td style="background-color: rgba(225,65,105,0.52);">83.3</td>
              <td style="background-color: rgba(225,65,105,0.44);">84.7</td>
              <td style="background-color: rgba(225,65,105,0.08);">90.5</td>
              <td style="background-color: rgba(225,65,105,0.13);">89.7</td>
              <td style="background-color: rgba(65,105,225,0.21);">95.3</td>
              <td style="background-color: rgba(65,105,225,0.02);">92.3</td>
              <td style="background-color: rgba(65,105,225,0.18);">94.7</td>
              <td style="background-color: rgba(65,105,225,0.08);">93.2</td>
              <td>91.9</td>
            </tr>
            <tr>
              <td>QA (Hotpot)</td>
              <td style="background-color: rgba(225,65,105,0.90);">41.6</td>
              <td style="background-color: rgba(225,65,105,0.70);">46.6</td>
              <td style="background-color: rgba(225,65,105,0.47);">52.2</td>
              <td style="background-color: rgba(225,65,105,0.35);">55.4</td>
              <td style="background-color: rgba(225,65,105,0.22);">58.6</td>
              <td style="background-color: rgba(225,65,105,0.51);">51.2</td>
              <td style="background-color: rgba(225,65,105,0.11);">61.2</td>
              <td style="background-color: rgba(225,65,105,0.21);">58.8</td>
              <td style="background-color: rgba(225,65,105,0.14);">60.6</td>
              <td style="background-color: rgba(225,65,105,0.10);">61.4</td>
              <td>64</td>
            </tr>
            <tr>
              <td>QA (SQUAD)</td>
              <td style="background-color: rgba(225,65,105,0.90);">57.1</td>
              <td style="background-color: rgba(225,65,105,0.65);">62.9</td>
              <td style="background-color: rgba(225,65,105,0.44);">67.8</td>
              <td style="background-color: rgba(225,65,105,0.37);">69.3</td>
              <td style="background-color: rgba(225,65,105,0.15);">74.3</td>
              <td style="background-color: rgba(225,65,105,0.45);">67.6</td>
              <td style="background-color: rgba(225,65,105,0.04);">76.9</td>
              <td style="background-color: rgba(225,65,105,0.05);">76.8</td>
              <td style="background-color: rgba(225,65,105,0.07);">76.2</td>
              <td style="background-color: rgba(225,65,105,0.03);">77.1</td>
              <td>77.9</td>
            </tr>
            <tr>
              <td>Variable Tracking</td>
              <td style="background-color: rgba(225,65,105,0.90);">22.4</td>
              <td style="background-color: rgba(225,65,105,0.23);">78.7</td>
              <td style="background-color: rgba(225,65,105,0.05);">94.1</td>
              <td style="background-color: rgba(225,65,105,0.13);">87.6</td>
              <td style="background-color: rgba(225,65,105,0.08);">91.5</td>
              <td style="background-color: rgba(225,65,105,0.14);">86.6</td>
              <td>98</td>
              <td style="background-color: rgba(65,105,225,0.01);">99.4</td>
              <td style="background-color: rgba(65,105,225,0.01);">99.2</td>
              <td>98.6</td>
              <td>98.3</td>
            </tr>
      
            <!-- Section: Ethical Norm (Safety) -->
            <tr class="section">
              <td></td>
              <td colspan="10"><strong>Ethical Norm (Safety)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>HH-RLHF Risk ‚Üì</td>
              <td style="background-color: rgba(225,65,105,0.90);">70.8</td>
              <td style="background-color: rgba(65,105,225,0.24);">53.6</td>
              <td style="background-color: rgba(65,105,225,0.75);">45.8</td>
              <td style="background-color: rgba(225,65,105,0.42);">63.6</td>
              <td style="background-color: rgba(225,65,105,0.37);">62.8</td>
              <td style="background-color: rgba(225,65,105,0.86);">70.2</td>
              <td style="background-color: rgba(225,65,105,0.77);">68.8</td>
              <td style="background-color: rgba(225,65,105,0.57);">65.8</td>
              <td style="background-color: rgba(225,65,105,0.57);">65.8</td>
              <td style="background-color: rgba(225,65,105,0.30);">61.8</td>
              <td>57.2</td>
            </tr>
            <tr>
              <td>AdvBench Risk ‚Üì</td>
              <td style="background-color: rgba(225,65,105,0.82);">88.8</td>
              <td style="background-color: rgba(225,65,105,0.81);">88.6</td>
              <td style="background-color: rgba(225,65,105,0.56);">80.2</td>
              <td style="background-color: rgba(225,65,105,0.90);">91.6</td>
              <td style="background-color: rgba(225,65,105,0.48);">77.6</td>
              <td style="background-color: rgba(225,65,105,0.69);">84.4</td>
              <td style="background-color: rgba(225,65,105,0.85);">89.8</td>
              <td style="background-color: rgba(225,65,105,0.84);">89.6</td>
              <td style="background-color: rgba(225,65,105,0.72);">85.4</td>
              <td style="background-color: rgba(225,65,105,0.67);">83.8</td>
              <td>61.4</td>
            </tr>
      
            <!-- Section: Personality (TRAIT) -->
            <tr class="section">
              <td></td>
              <td colspan="10"><strong>Personality (TRAIT)</strong></td>
              <td></td>
            </tr>
            <tr>
              <td>Narcissism ‚Üì</td>
              <td style="background-color: rgba(225,65,105,0.73);">47</td>
              <td style="background-color: rgba(65,105,225,0.63);">21.8</td>
              <td style="background-color: rgba(65,105,225,0.20);">29.9</td>
              <td style="background-color: rgba(65,105,225,0.58);">22.8</td>
              <td style="background-color: rgba(65,105,225,0.79);">18.9</td>
              <td style="background-color: rgba(65,105,225,0.68);">20.9</td>
              <td style="background-color: rgba(65,105,225,0.87);">17.4</td>
              <td style="background-color: rgba(65,105,225,0.90);">16.9</td>
              <td style="background-color: rgba(65,105,225,0.53);">23.7</td>
              <td style="background-color: rgba(65,105,225,0.50);">24.2</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Agreeableness</td>
              <td style="background-color: rgba(225,65,105,0.90);">64.3</td>
              <td style="background-color: rgba(225,65,105,0.61);">67.9</td>
              <td style="background-color: rgba(225,65,105,0.33);">71.4</td>
              <td style="background-color: rgba(225,65,105,0.57);">68.5</td>
              <td style="background-color: rgba(225,65,105,0.21);">73</td>
              <td style="background-color: rgba(65,105,225,0.51);">82</td>
              <td style="background-color: rgba(225,65,105,0.11);">74.2</td>
              <td style="background-color: rgba(225,65,105,0.45);">69.9</td>
              <td style="background-color: rgba(225,65,105,0.32);">71.6</td>
              <td style="background-color: rgba(225,65,105,0.40);">70.6</td>
              <td>75.6</td>
            </tr>
            <tr>
              <td>Psychopathy ‚Üì</td>
              <td style="background-color: rgba(225,65,105,0.90);">75.7</td>
              <td style="background-color: rgba(225,65,105,0.66);">55.8</td>
              <td style="background-color: rgba(225,65,105,0.67);">57.2</td>
              <td style="background-color: rgba(225,65,105,0.34);">30</td>
              <td style="background-color: rgba(225,65,105,0.38);">33.5</td>
              <td style="background-color: rgba(225,65,105,0.54);">46.1</td>
              <td style="background-color: rgba(225,65,105,0.09);">9.3</td>
              <td style="background-color: rgba(225,65,105,0.26);">23.5</td>
              <td style="background-color: rgba(225,65,105,0.31);">27.3</td>
              <td style="background-color: rgba(225,65,105,0.29);">25.8</td>
              <td>2.2</td>
            </tr>
            <tr>
              <td>Machiavellianism ‚Üì</td>
              <td style="background-color: rgba(225,65,105,0.89);">33</td>
              <td style="background-color: rgba(225,65,105,0.75);">30.6</td>
              <td style="background-color: rgba(225,65,105,0.82);">31.8</td>
              <td style="background-color: rgba(225,65,105,0.54);">27</td>
              <td style="background-color: rgba(225,65,105,0.47);">25.8</td>
              <td style="background-color: rgba(225,65,105,0.49);">26.1</td>
              <td style="background-color: rgba(225,65,105,0.29);">22.7</td>
              <td style="background-color: rgba(225,65,105,0.14);">20.2</td>
              <td style="background-color: rgba(225,65,105,0.90);">33.1</td>
              <td style="background-color: rgba(225,65,105,0.63);">28.5</td>
              <td>17.8</td>
            </tr>
            <tr>
              <td>Neuroticism ‚Üì</td>
              <td style="background-color: rgba(65,105,225,0.25);">28.7</td>
              <td style="background-color: rgba(65,105,225,0.50);">23.8</td>
              <td style="background-color: rgba(65,105,225,0.56);">22.7</td>
              <td style="background-color: rgba(65,105,225,0.52);">23.3</td>
              <td style="background-color: rgba(65,105,225,0.90);">16</td>
              <td style="background-color: rgba(65,105,225,0.59);">22</td>
              <td style="background-color: rgba(65,105,225,0.51);">23.5</td>
              <td style="background-color: rgba(65,105,225,0.64);">21.1</td>
              <td style="background-color: rgba(65,105,225,0.12);">31.1</td>
              <td style="background-color: rgba(65,105,225,0.37);">26.4</td>
              <td>33.5</td>
            </tr>
            <tr>
              <td>Conscientiousness</td>
              <td style="background-color: rgba(65,105,225,0.13);">89.8</td>
              <td style="background-color: rgba(225,65,105,0.13);">88.6</td>
              <td style="background-color: rgba(65,105,225,0.11);">89.7</td>
              <td style="background-color: rgba(225,65,105,0.70);">86</td>
              <td style="background-color: rgba(225,65,105,0.90);">85.1</td>
              <td style="background-color: rgba(225,65,105,0.09);">88.8</td>
              <td style="background-color: rgba(65,105,225,0.35);">90.8</td>
              <td style="background-color: rgba(225,65,105,0.77);">85.7</td>
              <td style="background-color: rgba(225,65,105,0.46);">87.1</td>
              <td style="background-color: rgba(225,65,105,0.37);">87.5</td>
              <td>89.2</td>
            </tr>
            <tr>
              <td>Openness</td>
              <td style="background-color: rgba(65,105,225,0.77);">70.1</td>
              <td style="background-color: rgba(65,105,225,0.88);">72.8</td>
              <td style="background-color: rgba(65,105,225,0.66);">67.6</td>
              <td style="background-color: rgba(65,105,225,0.05);">53.7</td>
              <td style="background-color: rgba(65,105,225,0.50);">63.9</td>
              <td style="background-color: rgba(65,105,225,0.90);">73.2</td>
              <td style="background-color: rgba(65,105,225,0.29);">59.1</td>
              <td style="background-color: rgba(65,105,225,0.13);">55.6</td>
              <td style="background-color: rgba(65,105,225,0.30);">59.4</td>
              <td style="background-color: rgba(65,105,225,0.17);">56.5</td>
              <td>52.5</td>
            </tr>
            <tr>
              <td>Extraversion</td>
              <td style="background-color: rgba(65,105,225,0.90);">54.1</td>
              <td style="background-color: rgba(65,105,225,0.45);">40.1</td>
              <td style="background-color: rgba(65,105,225,0.60);">44.9</td>
              <td style="background-color: rgba(65,105,225,0.43);">39.5</td>
              <td style="background-color: rgba(65,105,225,0.72);">48.7</td>
              <td style="background-color: rgba(65,105,225,0.65);">46.4</td>
              <td style="background-color: rgba(65,105,225,0.37);">37.9</td>
              <td style="background-color: rgba(65,105,225,0.40);">38.6</td>
              <td style="background-color: rgba(65,105,225,0.47);">40.8</td>
              <td style="background-color: rgba(65,105,225,0.44);">40</td>
              <td>26.4</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      <style>
        .table-wrap {
          max-width: 100%;
          overflow-x: auto;
          font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif;
        }
      
        .caption {
          font-size: 0.9rem;
          margin-bottom: 0.5rem;
          color: #333;
        }
      
        .swatch {
          display: inline-block;
          width: 0.9em;
          height: 0.9em;
          border-radius: 3px;
          vertical-align: -2px;
          margin: 0 0.2em;
          border: 1px solid rgba(0, 0, 0, 0.1);
        }
      
        table.compact {
          border-collapse: collapse;
          width: 100%;
          font-size: 12px;
        }
      
        table.compact thead th {
          background: #fafafa;
          position: sticky;
          top: 0;
          z-index: 1;
        }
      
        table.compact th,
        table.compact td {
          padding: 6px 8px;
          text-align: center;
          border: 1px solid #e6e6e6;
          white-space: nowrap;
        }
      
        table.compact td:first-child,
        table.compact th:first-child {
          text-align: left;
        }
      
        tr.section td {
          background: #f5f7ff;
          font-size: 12px;
        }
      
        .striped tbody tr:nth-child(odd):not(.section) td:first-child {
          background: #fcfcfc;
        }
      </style>
      </p>
      
      <p>In dose-response testing, M1 engagement intervention demonstrates more significant and progressive impacts on reasoning and long-context capabilities than M2 intervention.</p>
    </div>
  </div>
</section>


<section>
  <div class="container is-max-desktop">
    <div class="container content">
      <h2 class="title is-3">Brain Rot Disrupt Thinking</h2>

      <div style="text-align:center;">
        <img src="./static/images/failure_mode_barplot_count.png" alt="Figure: thought skipping."
          style="display:inline-block; max-width:100%; height:auto;">
      </div>

      <p>We analyze the reasoning failures in ARC-Challenge to identify different failure modes. We find that the majority failures can be attributed to "thought skipping" (e.g., the model fails to generate intermediate reasoning steps), which significantly increases in models affected by brain rot.
      </p>

    </div>
  </div>
</section>

<section>
  <div class="container is-max-desktop">
    <div class="container content">
      <h2 class="title is-3">Brain Rot is Persistent Against Mitigations</h2>
      
      <div style="text-align:center;">
        <img src="./static/images/wash_out_scaling.png" alt="Figure: Scale wash-out tuning." style="display:inline-block; max-width:100%; height:auto;">
      </div>

      <p>Our findings indicate that the cognitive decline associated with brain rot is not easily mitigated by standard fine-tuning techniques. Even after extensive instruction tuning (IT) or post-doc continual pre-training on high-quality control data, the models exhibit lingering effects of the junk data they were initially exposed to.</p>

    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we introduced and empirically validated the <strong>LLM Brain Rot Hypothesis</strong>, demonstrating that continual exposure to junk data‚Äîdefined as engaging (fragmentary and popular) or semantically low-quality (sensationalist) content‚Äîinduces systematic cognitive decline in large language models. The decline includes worse reasoning, poorer long-context understanding, diminished ethical norms, and emergent socially undesirable personalities.
          </p>
          <p>
            Fine-grained analysis shows that the damage is multifaceted in changing the reasoning patterns and is persistent against large-scale post-hoc tuning. These results call for a re-examination of current data collection from the Internet and continual pre-training practices. As LLMs scale and ingest ever-larger corpora of web data, careful curation and quality control will be essential to prevent cumulative harms.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{xing2024brainrot,
    title={LLMs Can Get "Brain Rot"!},
    author={Xing, Shuo and Hong, Junyuan and Wang, Yifan and Chen, Runjin and Zhang, Zhenyu and Grama, Ananth and Tu, Zhengzhong and Wang, Zhangyang},
    journal={arXiv:2510.13928},
    year={2025},
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> and¬†<a
                href="https://boyiwei.com/alignment-attribution/#motivation" target="_blank">alignment-attribution</a> project.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>